{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c69d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# copy from https://github.com/LeeSinLiang/microGPT/blob/ed40cf9780dbeb180adfe94c227d4aa97e69250e/gpt.py\n",
    "def top_k_top_p_filter(logits: torch.Tensor, top_k: int = 0, top_p: float = 0.0):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensorpe_): 2D tensor with shape (batch, vocab)\n",
    "        top_k (int, optional): top_k. Defaults to 0.\n",
    "        top_p (float, optional): top_p. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: a renormalized logits\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        filter = torch.topk(logits, min(top_k, logits.size(-1)))[0]\n",
    "        logits[logits < filter[:, [-1]]] = float('-inf')\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(\n",
    "            F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        filter = cumulative_probs > top_p\n",
    "        filter[..., 1:] = filter[..., :-1].clone()\n",
    "        filter[..., 0] = 0\n",
    "        indices_to_remove = filter.scatter(1, sorted_indices, filter)\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "\n",
    "def norm_logits(logits : torch.Tensor, temperature : float, top_k : float, top_p : float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): shape (1, vocab)\n",
    "        temperature (float): temperature\n",
    "        top_k (float): top_k\n",
    "        top_p (float): top_p\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: next token with shape as (batch,  1)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 2\n",
    "    logits = logits / temperature\n",
    "    logits = top_k_top_p_filter(logits, top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def sample(probs : torch.Tensor, num_samples: int = 1):\n",
    "    idx_next = torch.multinomial(probs, num_samples=num_samples)\n",
    "    if (idx_next.item() == 0):\n",
    "        raise RuntimeError\n",
    "    return idx_next\n",
    "\n",
    "\n",
    "def max_fn(x):\n",
    "    \"\"\"\n",
    "        norm(max (x, 0))\n",
    "    \"\"\"\n",
    "    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    x_max_sum = torch.sum(x_max, dim=1, keepdim=True) \n",
    "    return x_max / x_max_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21f87f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "model = Mamba.from_pretrained('state-spaces/mamba-370m')\n",
    "assistant_model = Mamba.from_pretrained('state-spaces/mamba-130m')\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a6928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_sampling(x : torch.Tensor, model : torch.nn.Module, N : int, \n",
    "                            temperature : float = 1, top_k : int = 0, top_p : float = 0):\n",
    "    n = len(x)\n",
    "    T = len(x) + N\n",
    "\n",
    "    while n < T:\n",
    "        # outputs = model(x)\n",
    "        outputs = model(x)\n",
    "        last_p = norm_logits(outputs[::, -1, :], temperature, top_k, top_p)\n",
    "        idx_next = sample(last_p)\n",
    "        x = torch.cat((x, idx_next), dim=1)\n",
    "        n += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6694c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The meaning of life is '\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27c401ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_p = 0.9\n",
    "num_tokens = 20\n",
    "\n",
    "torch.manual_seed(123)\n",
    "autoregressive_output = autoregressive_sampling(input_ids, model, num_tokens, top_k = top_k, top_p=top_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f6f8ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is \\nthe ultimate goal of your life \\nand the end of your life. \\nAnd so'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_completions = [tokenizer.decode(output.tolist()) for output in autoregressive_output][0]\n",
    "output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7635abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "@torch.no_grad()\n",
    "def speculative_sampling_v2(prefix : torch.Tensor, approx_model : torch.nn.Module, target_model : torch.nn.Module, \n",
    "                         max_len : int , gamma : int = 4,\n",
    "                         temperature : float = 1, top_k : int = 0, top_p : float = 0, random_seed : int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    DeepMind version Speculative Sampling.\n",
    "    Accelerating Large Language Model Decoding with Speculative Sampling\n",
    "    https://arxiv.org/abs/2302.01318\n",
    "    No KV Cache Optimization\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): input sequence, (batch, prefix_seqlen), Note that the batch dim is always 1 now.\n",
    "        approx_model (torch.nn.Module): approx model, the small one\n",
    "        target_model (torch.nn.Module): target model, the large one\n",
    "        max_len (int): the max overall generated tokens number.\n",
    "        gamma (int): $\\gamma$, the token number small model guesses.\n",
    "        temperature (float, optional): Defaults to 1.\n",
    "        top_k (int, optional): Defaults to 0.\n",
    "        top_p (float, optional): Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: generated tokens (batch, target_seqlen)\n",
    "    \"\"\"\n",
    "    seq_len = prefix.shape[1]\n",
    "    T = seq_len + max_len\n",
    "    \n",
    "    assert prefix.shape[0] == 1, \"input batch size must be 1\"\n",
    "\n",
    "    while prefix.shape[1] < T:\n",
    "        # q = M_q[prefix + x_0, x_1, .., x_(gamma-2)]\n",
    "        x = prefix\n",
    "        prefix_len = prefix.shape[1]\n",
    "        for _ in range(gamma):\n",
    "            # p.logits shape (batch, seq, vocab)\n",
    "            q = approx_model(x)\n",
    "            next_tok = sample(norm_logits(q[:, -1, :], \n",
    "                              temperature, top_k, top_p))\n",
    "            x = torch.cat((x, next_tok), dim=1)\n",
    "\n",
    "        # normalize the logits\n",
    "        for i in range(q.shape[1]):\n",
    "            q[:,i,:] = norm_logits(q[:,i,:],\n",
    "                            temperature, top_k, top_p)\n",
    "        # p  = M_p[prefix + x_0, x_0, .., x_(gamma-1)]\n",
    "        p = target_model(x)\n",
    "        for i in range(p.shape[1]):\n",
    "            p[:,i,:] = norm_logits(p[:,i,:],\n",
    "                            temperature, top_k, top_p)\n",
    "\n",
    "        # n the end position of the valid prefix\n",
    "        # x = x_[:prefix_len-1] + x_0, ... x_(gamma-1)\n",
    "\n",
    "        is_all_accept = True\n",
    "        n = prefix_len - 1\n",
    "        for i in range(gamma):\n",
    "            if random_seed:\n",
    "                torch.manual_seed(random_seed)\n",
    "            r = torch.rand(1, device = p.device)\n",
    "            j = x[:, prefix_len + i]\n",
    "\n",
    "            if r < torch.min(torch.tensor([1], device=q.device), p[:, prefix_len + i - 1, j] / q[:, prefix_len + i - 1, j]):\n",
    "                # accept, and update n\n",
    "                n += 1\n",
    "            else:\n",
    "                # reject\n",
    "                t = sample(max_fn(p[:, n, :] - q[:, n, :]))\n",
    "                is_all_accept = False\n",
    "                break\n",
    "\n",
    "        prefix = x[:, :n + 1]\n",
    "\n",
    "        if is_all_accept:\n",
    "            t = sample(p[:, -1, :])\n",
    "\n",
    "        prefix = torch.cat((prefix, t), dim=1)\n",
    "\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10875fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is 不會 Any time the word “ someday “ is substituted by the word “ is … No need'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speculative_output = speculative_sampling_v2(input_ids, assistant_model, model, num_tokens)\n",
    "output_completions = [tokenizer.decode(output.tolist()) for output in speculative_output][0]\n",
    "output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acd11404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  510,  4495,   273,  1495,   310,   209,   187, 12563,   281,  4264,\n",
       "          1495,    15,   187,   187,   395,   187,   187,  5288,  1539,  2637,\n",
       "          3481, 49367,   187, 50276,  4527,  8057,   272]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d531efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 510, 4495,  273, 1495,  310,  209]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7ee42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
